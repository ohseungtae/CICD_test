name: CI/CD Pipeline
on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

env:
  ENABLE_MLFLOW: false
  CI: true
#.
jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, '3.10']

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-mock

    - name: Create necessary directories
      run: |
        mkdir -p dataset
        mkdir -p models
        mkdir -p results

    - name: Run tests with environment variables
      env:
        PYTHONPATH: .
        ENABLE_MLFLOW: false
        CI: true
      run: pytest tests/ -v --cov=src --cov-report=xml

    - name: Lint with flake8
      run: |
        pip install flake8
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  integration-test:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'pull_request' || (github.event_name == 'push' && contains(fromJson('["refs/heads/main", "refs/heads/develop"]'), github.ref))

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Create necessary directories
      run: |
        mkdir -p dataset
        mkdir -p models
        mkdir -p results

    - name: Set environment variables for CI
      run: |
        echo "ENABLE_MLFLOW=false" >> $GITHUB_ENV
        echo "CI=true" >> $GITHUB_ENV
        echo "PYTHONPATH=." >> $GITHUB_ENV

    - name: Generate test data
      run: |
        python -c "
        import pandas as pd
        import numpy as np
        from datetime import datetime, timedelta
        import os
        
        # utils.pyì™€ ì¼ì¹˜í•˜ëŠ” ë””ë ‰í† ë¦¬ êµ¬ì¡° ì‚¬ìš©
        os.makedirs('dataset', exist_ok=True)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='H')
        temps = 15 + 10 * np.sin(2 * np.pi * np.arange(len(dates)) / (24*365)) + np.random.normal(0, 2, len(dates))
        test_data = pd.DataFrame({'time': dates, 'temp': temps})
        test_data.to_csv('dataset/test_weather.csv', index=False)
        
        # ë°ì´í„° ë¶„í• 
        split_idx = int(len(test_data) * 0.8)
        train_data = test_data[:split_idx]
        test_data_split = test_data[split_idx:]
        
        train_data.to_csv('dataset/train_data.csv', index=False)
        test_data_split.to_csv('dataset/test_data.csv', index=False)
        
        print('Test data created successfully')
        print(f'Train data shape: {train_data.shape}')
        print(f'Test data shape: {test_data_split.shape}')
        "

    - name: Debug directory structure
      run: |
        echo "Current working directory: $(pwd)"
        echo "Project structure:"
        find . -type d -name "dataset" -o -name "models" 2>/dev/null || echo "Directories not found"
        echo "Contents of dataset/:"
        ls -la dataset/ || echo "dataset/ directory not found"
        echo "Contents of models/:"
        ls -la models/ || echo "models/ directory not found"
        
        python -c "
        import sys
        sys.path.insert(0, '.')
        from src.utils.utils import model_dir, dataset_dir
        print(f'model_dir() returns: {model_dir()}')
        print(f'dataset_dir() returns: {dataset_dir()}')
        "

    - name: Test Prophet training
      run: |
        python -c "
        import sys
        import os
        sys.path.insert(0, '.')
        os.environ['ENABLE_MLFLOW'] = 'false'
        
        from src.train.train import train_prophet
        from src.utils.utils import ensure_dir, model_dir
        
        print('Starting Prophet model training...')
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì‹¤íˆ ìƒì„±
        model_directory = model_dir()
        ensure_dir(model_directory)
        print(f'Model directory ensured: {model_directory}')
        
        model_path, run_id = train_prophet('dataset/train_data.csv')
        print(f'Model trained successfully: {model_path}')
        print(f'Run ID: {run_id}')
        
        # ëª¨ë¸ íŒŒì¼ì´ ì‹¤ì œë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if os.path.exists(model_path):
            print(f'âœ“ Model file confirmed at: {model_path}')
            print(f'File size: {os.path.getsize(model_path)} bytes')
        else:
            print(f'âœ— ERROR: Model file not found at {model_path}')
            # ì „ì²´ í”„ë¡œì íŠ¸ì—ì„œ pkl íŒŒì¼ ì°¾ê¸°
            import glob
            pkl_files = glob.glob('**/*.pkl', recursive=True)
            print(f'All .pkl files in project: {pkl_files}')
            sys.exit(1)
        "

    - name: Test Prophet evaluation
      run: |
        python -c "
        import sys
        import os
        sys.path.insert(0, '.')
        os.environ['ENABLE_MLFLOW'] = 'false'
        
        from src.evaluate.evaluate import evaluate_prophet
        from src.utils.utils import model_dir
        
        print('Starting Prophet model evaluation...')
        
        # ëª¨ë¸ ê²½ë¡œë¥¼ utils í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ ê°€ì ¸ì˜¤ê¸°
        model_path = model_dir('prophet_model.pkl')
        print(f'Looking for model at: {model_path}')
        
        if not os.path.exists(model_path):
            print(f'ERROR: Model file not found at {model_path}')
            # ë””ë²„ê¹…ì„ ìœ„í•´ í˜„ì¬ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì¶œë ¥
            import glob
            print('Current working directory:', os.getcwd())
            print('All pkl files in project:')
            for pkl_file in glob.glob('**/*.pkl', recursive=True):
                print(f'  - {pkl_file}')
            sys.exit(1)
        
        metrics = evaluate_prophet(model_path, 'dataset/test_data.csv', 'mock_run_id')
        print(f'Evaluation completed: MAE={metrics[\"mae\"]:.2f}, RMSE={metrics[\"rmse\"]:.2f}')
        "

    - name: Test SARIMAX training
      run: |
        python -c "
        import sys
        import os
        sys.path.insert(0, '.')
        os.environ['ENABLE_MLFLOW'] = 'false'
        
        from src.train.train import train_sarimax
        from src.utils.utils import ensure_dir, model_dir
        
        print('Starting SARIMAX model training...')
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì‹¤íˆ ìƒì„±
        model_directory = model_dir()
        ensure_dir(model_directory)
        
        model_path, run_id = train_sarimax('dataset/train_data.csv')
        print(f'SARIMAX model trained successfully: {model_path}')
        print(f'Run ID: {run_id}')
        
        if os.path.exists(model_path):
            print(f'âœ“ SARIMAX model file confirmed at: {model_path}')
        else:
            print(f'âœ— ERROR: SARIMAX model file not found at {model_path}')
            sys.exit(1)
        "

    - name: Test SARIMAX evaluation
      run: |
        python -c "
        import sys
        import os
        sys.path.insert(0, '.')
        os.environ['ENABLE_MLFLOW'] = 'false'
        
        from src.evaluate.evaluate import evaluate_sarimax
        from src.utils.utils import model_dir
        
        print('Starting SARIMAX model evaluation...')
        
        model_path = model_dir('sarimax_model.pkl')
        print(f'Looking for SARIMAX model at: {model_path}')
        
        if not os.path.exists(model_path):
            print(f'ERROR: SARIMAX model file not found at {model_path}')
            sys.exit(1)
        
        metrics = evaluate_sarimax(model_path, 'dataset/test_data.csv', 'mock_run_id')
        print(f'SARIMAX evaluation completed: MAE={metrics[\"mae\"]:.2f}, RMSE={metrics[\"rmse\"]:.2f}')
        "

    - name: Test Future prediction
      run: |
        python -c "
        import sys  
        import os
        sys.path.insert(0, '.')
        os.environ['ENABLE_MLFLOW'] = 'false'
        
        from src.test.test import predict_future
        from src.utils.utils import model_dir
        import pandas as pd
        import joblib
        
        print('Starting future prediction test...')
        
        # ëª¨ë¸ ê²½ë¡œë¥¼ utils í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ ê°€ì ¸ì˜¤ê¸°
        model_path = model_dir('prophet_model.pkl')
        print(f'Loading model from: {model_path}')
        
        model = joblib.load(model_path)
        test_data = pd.read_csv('dataset/test_data.csv', parse_dates=['time'])
        last_date = test_data['time'].max()
        
        print(f'Last date in test data: {last_date}')
        print('Generating 3-day forecast...')
        
        future_csv = predict_future(model, last_date, days=3, save_name='test_forecast.csv')
        print(f'Future prediction completed: {future_csv}')
        
        # ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸
        if os.path.exists(future_csv):
            forecast_df = pd.read_csv(future_csv)
            print(f'Forecast data shape: {forecast_df.shape}')
            print('First few predictions:')
            print(forecast_df.head())
        "

  build-and-deploy:
    runs-on: ubuntu-latest
    needs: [test, integration-test]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Cache Docker layers
      uses: actions/cache@v3
      with:
        path: /tmp/.buildx-cache
        key: ${{ runner.os }}-buildx-${{ github.sha }}
        restore-keys: |
          ${{ runner.os }}-buildx-

    - name: Login to DockerHub
      uses: docker/login-action@v3
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ost0451/mlops_dev
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=local,src=/tmp/.buildx-cache
        cache-to: type=local,dest=/tmp/.buildx-cache-new,mode=max
        platforms: linux/amd64,linux/arm64

    - name: Move cache
      run: |
        rm -rf /tmp/.buildx-cache
        mv /tmp/.buildx-cache-new /tmp/.buildx-cache

    - name: Create deployment package
      run: |
        mkdir -p deployment
        cp -r src deployment/
        cp main.py deployment/
        cp requirements.txt deployment/
        cp config.yaml deployment/ || echo "config.yaml not found, skipping"
        cp Dockerfile deployment/ || echo "Dockerfile not found, skipping"

    - name: Archive deployment artifacts
      uses: actions/upload-artifact@v4
      with:
        name: deployment-package
        path: deployment/

    - name: Build completion notification
      run: |
        echo "ğŸš€ Docker image built and pushed successfully!"
        echo "ğŸ“¦ Image tags: ${{ steps.meta.outputs.tags }}"
        echo "ğŸ”— Docker Hub: https://hub.docker.com/r/ost0451/mlops_dev"
        echo "ğŸ“ Deployment artifacts uploaded and ready!"
        echo "âœ… CI/CD Pipeline completed successfully!"